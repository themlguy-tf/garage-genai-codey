{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wedn1zwCCUM_"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HN5ByYVLl4b"
      },
      "source": [
        "# Vertex AI Codey API\n",
        "\n",
        "This notebook shows a few examples of using Codey via the latest Vertex AI SDK.\n",
        "\n",
        "What are LLMs\n",
        "Large language models (LLMs) are deep learning models trained on massive datasets of text. LLMs can translate language, summarize text, generate creative writing, generate code, power chatbots and virtual assistants, and complement search engines and recommendation systems. Creating an LLM requires massive amounts of data, significant compute resources, and specialized skills. Because LLMs require a big investment to create, they target broad rather than specific use cases. On Vertex AI, you can customize a foundation model for more specific tasks or knowledge domains by using prompt design and model tuning.\n",
        "The Vertex AI PaLM API enables you to test, customize, and deploy instances of Google’s large language models (LLM) so that you can leverage the capabilities of PaLM in your applications.\n",
        "PaLM 2 is Google's next generation Large Language Model (LLM) that builds on Google's legacy of breakthrough research in machine learning and responsible AI. PaLM 2 excels at tasks like advanced reasoning, translation, and code generation because of how it was built.\n",
        "Generative AI Studio and the Vertex AI PaLM API is powered by PaLM 2.\n",
        "Objectives:\n",
        "\n",
        "✅ Introduce Vertex PaLM API Text\n",
        "\n",
        "✅ Introduce Codey PaLM API components\n",
        "\n",
        "✅ Codey generated code samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIXvyM6RHZJ"
      },
      "source": [
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaOBG4Srn48k"
      },
      "source": [
        "Install the latest Vertex AI SDK for python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP9JbFLbn8le"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform --upgrade --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twqcY0Uon-_Q"
      },
      "source": [
        "---\n",
        "\n",
        "#### ⚠️ Do not forget to click the \"RESTART RUNTIME\" button above.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3jBn1Zen2Xz"
      },
      "source": [
        "First let's set the GCP Project ID and use this to initialize the Vertex AI SDK. Please enter the project id and location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amN4OqML-0a",
        "outputId": "36755828-da2b-441f-beda-766ab2c4cf2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertex AI SDK version: 1.30.1 initialized!\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"<PLEASE ENTER THE PROJECT ID>\" #@param {type:\"string\"}\n",
        "LOCATION = \"<PLEASE ENTER THE LOCATION>\" #\"us-central1\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2uABPTOkIKZ"
      },
      "source": [
        "## Import the required libararies and initialize the sdk using project and location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd--Lx3QkIKZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.language_models._language_models import TextGenerationModel, ChatModel, CodeChatModel, CodeGenerationModel\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__} initialized!\")\n",
        "\n",
        "from vertexai.language_models._language_models import CodeGenerationModel\n",
        "import IPython.display as display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99xuHiYoI11"
      },
      "source": [
        "## Let's use codey to generate code!\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚠️ You are using the model - code-bison@001.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lYXq1MvXo24i"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"write a go function that reverses a string\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "0acE2b11oB0W",
        "outputId": "5d61ce90-c8dd-4d81-8da2-1e7fe4c93650"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```go\n",
              "func ReverseString(s string) string {\n",
              "  r := []rune(s)\n",
              "  for i, j := 0, len(r)-1; i < j; i, j = i+1, j-1 {\n",
              "    r[i], r[j] = r[j], r[i]\n",
              "  }\n",
              "  return string(r)\n",
              "}\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from vertexai.language_models._language_models import CodeGenerationModel\n",
        "import IPython.display as display\n",
        "\n",
        "codegen = CodeGenerationModel.from_pretrained(\"code-bison@001\")\n",
        "\n",
        "result = codegen.predict(\n",
        "    PROMPT,\n",
        "    # Optional:\n",
        "    max_output_tokens=128,\n",
        "    temperature=0.65,\n",
        "    #top_p=1,\n",
        "    #top_k=5,\n",
        ")\n",
        "\n",
        "display.Markdown(result.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPeMz92WKag"
      },
      "source": [
        "## Let's use codey to write a unit test for a function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCbUrcSRWL8n",
        "outputId": "eedf6acd-4276-4ff5-bf4f-0b59d845fd95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "def test_is_leap_year():\n",
            "  # Check that the function returns True for known leap years.\n",
            "  assert is_leap_year(2020) == True\n",
            "  assert is_leap_year(2000) == True\n",
            "  assert is_leap_year(1600) == True\n",
            "\n",
            "  # Check that the function returns False for known non-leap years.\n",
            "  assert is_leap_year(2019) == False\n",
            "  assert is_leap_year(1900) == False\n",
            "  assert is_leap_year(1700) == False\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "PROMPT = \"\"\"Write a unit test for this function\n",
        "def is_leap_year(year):\n",
        "  if year % 4 == 0:\n",
        "    if year % 100 == 0:\n",
        "      if year % 400 == 0:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    else:\n",
        "      return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "response = codegen.predict(\n",
        "    PROMPT,\n",
        "    # Optional:\n",
        "    max_output_tokens=654,\n",
        "    temperature=0.65,\n",
        "    #top_p=1,\n",
        "    #top_k=5,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd86dwD3UHBv"
      },
      "source": [
        "## Another Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "bKl4jBnMUGmW",
        "outputId": "e0ee9803-23ee-413d-ae73-8071aa05c37c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```go\n",
              "package main\n",
              "\n",
              "import (\n",
              "    \"encoding/json\"\n",
              "    \"fmt\"\n",
              "    \"net/http\"\n",
              ")\n",
              "\n",
              "// HandlerFunc is a function that handles HTTP requests.\n",
              "type HandlerFunc func(w http.ResponseWriter, r *http.Request)\n",
              "\n",
              "// HelloHandler is a handler that returns a greeting to the user.\n",
              "func HelloHandler(w http.ResponseWriter, r *http.Request) {\n",
              "    // Decode the JSON message.\n",
              "    var name struct {\n",
              "        Name string `json:\"name\"`\n",
              "    }\n",
              "    if err := json.NewDecoder(r.Body).Decode(&name); err != nil {\n",
              "        http.Error(w, err.Error(), http.StatusBadRequest)\n",
              "        return\n",
              "    }\n",
              "\n",
              "    // Prepend \"hello \" to the name.\n",
              "    name.Name = \"hello \" + name.Name\n",
              "\n",
              "    // Marshal the name back to JSON.\n",
              "    json.NewEncoder(w).Encode(name)\n",
              "}\n",
              "\n",
              "// main is the entry point for the program.\n",
              "func main() {\n",
              "    // Create a new HTTP server.\n",
              "    http.HandleFunc(\"/hello\", HelloHandler)\n",
              "    server := http.ListenAndServe(\":8080\", nil)\n",
              "}\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PROMPT = \"\"\"Write an http server in Go that takes a JSON message as a POST, extracts the name, and returns it, prepending \"hello \".\n",
        "Make sure the HTTP handler is in a separate method.\n",
        "Here is an example JSON:\n",
        "\n",
        "{\"name\":\"squirrel\"}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = codegen.predict(\n",
        "    PROMPT,\n",
        "    # Optional:\n",
        "    max_output_tokens=654,\n",
        "    temperature=0.65,\n",
        "    #top_p=1,\n",
        "    #top_k=5,\n",
        ")\n",
        "\n",
        "\n",
        "display.Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeN2xhSRcyft"
      },
      "source": [
        "## Let's try chat codey to generate code!\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚠️ You are using the model - codechat-bison@001.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_2GRbQ7c4Hq",
        "outputId": "67225044-4ed1-4e98-ab5b-5970da58b485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from Model: I am doing well, thank you for asking! I am excited to be learning more about natural language processing and how to use it to help people.\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
        "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "\n",
        "\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\"\"\"Hi, how are you?\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWi7oEL8kIKc",
        "outputId": "4bd342d1-7c0a-4989-c2d8-ca1a0729e84e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from Model: Sure, here is a function to calculate the minimum of two numbers:\n",
            "\n",
            "```\n",
            "def min(a, b):\n",
            "  \"\"\"\n",
            "  Calculates the minimum of two numbers.\n",
            "\n",
            "  Args:\n",
            "    a: The first number.\n",
            "    b: The second number.\n",
            "\n",
            "  Returns:\n",
            "    The smaller of the two numbers.\n",
            "  \"\"\"\n",
            "\n",
            "  if a < b:\n",
            "    return a\n",
            "  else:\n",
            "    return b\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"\"\"Please help write a function to calculate the min of two numbers\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zluOD5a8kIKc",
        "outputId": "93300c5d-fd78-4708-bb30-1096efb573de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from Model: Sure, here is the function with the variable names changed:\n",
            "\n",
            "```\n",
            "def min(first_num, second_num):\n",
            "  \"\"\"\n",
            "  Calculates the minimum of two numbers.\n",
            "\n",
            "  Args:\n",
            "    first_num: The first number.\n",
            "    second_num: The second number.\n",
            "\n",
            "  Returns:\n",
            "    The smaller of the two numbers.\n",
            "  \"\"\"\n",
            "\n",
            "  if first_num < second_num:\n",
            "    return first_num\n",
            "  else:\n",
            "    return second_num\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"\"\"Please change the previous function variables from a to first_num and b to second_num\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m110",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}